# Database Storage Setup Guide

## What Changed

Your extension now **automatically saves everything to PostgreSQL database**:

âœ… **Scrapers** - All created scrapers persisted
âœ… **Templates** - Custom templates saved
âœ… **RAG Memory** - Learning episodes with embeddings
âœ… **Domain Knowledge** - Patterns and successful techniques

**No more lost data!** Everything is backed up to database + localStorage.

## Quick Start

### 1. Start the API Server

```bash
# Option A: Double-click
start-api.bat

# Option B: Command line
cd scraper-backend
npm run api
```

**Server starts on port 3001** and provides:
- `/api/scrapers` - Save/load scrapers
- `/api/templates` - Template management  
- `/api/rag/episode` - RAG memory sync
- `/api/execute` - Script execution (moved from port 3002)

### 2. (Optional) Set Up PostgreSQL

If you want true persistence, configure PostgreSQL:

#### Option A: Local PostgreSQL
```bash
# Install PostgreSQL
https://www.postgresql.org/download/windows/

# Create database
psql -U postgres
CREATE DATABASE civitracker;
\q
```

#### Option B: Cloud PostgreSQL (Free)
1. Go to https://neon.tech or https://supabase.com
2. Create free database
3. Copy connection string

### 3. Configure Environment

Create `scraper-backend/.env`:
```env
# Database (optional - works without it!)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=civitracker
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password
POSTGRES_SSL=false

# API
API_PORT=3001
```

### 4. Reload Extension

1. Go to `chrome://extensions`
2. Click reload on your extension
3. Open extension - it will connect automatically

## How It Works

### Without Database
âœ… Extension works perfectly
âœ… Saves to localStorage (browser storage)
âœ… RAG memory works (keyword search)
âœ… Scrapers persist in browser
âŒ No cross-device sync
âŒ Data lost if browser cache cleared

### With Database
âœ… Everything from above, PLUS:
âœ… Cross-device sync
âœ… Permanent storage (never lost)
âœ… Better RAG search (vector embeddings)
âœ… Backup and restore
âœ… Analytics and reporting

## Automatic Syncing

The extension **automatically** syncs to database:

**When you create a scraper:**
```javascript
// Extension automatically calls:
await fetch('http://localhost:3001/api/scrapers', {
  method: 'POST',
  body: JSON.stringify(scraperConfig)
});
```

**After every generation:**
```javascript
// RAG memory automatically syncs:
await fetch('http://localhost:3001/api/rag/episode', {
  method: 'POST',
  body: JSON.stringify({
    domain, script, testResult, embedding, summary
  })
});
```

**No manual work required!**

## Database Schema

The API server **automatically creates** these tables:

### `scraper_configs`
Stores all your created scrapers:
```sql
id SERIAL PRIMARY KEY
name VARCHAR(255)           -- "California Courts Scraper"
url TEXT                    -- "https://courts.ca.gov/calendar"
template_name VARCHAR(255)  -- "Court Calendar"
fields JSONB                -- { "step2-case_number": ".case-num", ... }
ai_fields JSONB             -- AI analysis configs
script_code TEXT            -- Generated scraper code
test_result JSONB           -- Last test results
created_at TIMESTAMP
updated_at TIMESTAMP
```

### `templates`
Stores custom templates:
```sql
id SERIAL PRIMARY KEY
name VARCHAR(255) UNIQUE
description TEXT
steps JSONB                 -- Template configuration
storage JSONB              -- Metadata
created_at TIMESTAMP
```

### `rag_episodes`
RAG learning memory:
```sql
id SERIAL PRIMARY KEY
domain VARCHAR(255)        -- "alamogordo-nm.gov"
template_type VARCHAR(255) -- "City Council Calendar"
url TEXT
success BOOLEAN
script TEXT
test_result JSONB
diagnosis JSONB
embedding FLOAT8[]         -- 512-dimensional vector
summary TEXT
created_at TIMESTAMP
```

### `rag_domain_concepts`
Domain-specific learning:
```sql
domain VARCHAR(255) PRIMARY KEY
success_count INTEGER
failure_count INTEGER
common_selectors JSONB    -- { ".calendar-item": 8, ".event-title": 7 }
common_tools JSONB        -- { "page.goto": 10, "page.waitFor": 8 }
common_issues JSONB
updated_at TIMESTAMP
```

## API Endpoints

### Scrapers
- `POST /api/scrapers` - Create scraper
- `PUT /api/scrapers/:id` - Update scraper
- `GET /api/scrapers/list` - Get all scrapers
- `DELETE /api/scrapers/:id/delete` - Delete scraper

### Templates
- `GET /api/templates` - Get all templates
- `POST /api/templates` - Save template

### RAG Memory
- `POST /api/rag/episode` - Save episode
- `GET /api/rag/episodes` - Get episodes

### Execution
- `POST /api/execute` - Execute scraper script

## Troubleshooting

### "Database not available - using localStorage"
**This is normal if:**
- API server not running
- PostgreSQL not configured
- First time setup

**Extension still works perfectly!** Just using browser storage.

**To fix:**
1. Run `start-api.bat`
2. Check server is running: http://localhost:3001/api/scrapers/list
3. Reload extension

### "Connection refused"
API server not running. Start it with `start-api.bat`.

### "Database connection failed"
PostgreSQL not configured or not running. Options:
1. **Use without database** - Extension works fine!
2. **Set up PostgreSQL** - Follow setup instructions above

### Lost scrapers after browser restart
This happens when:
- API server was never run (no DB sync)
- Browser cache was cleared

**Solution**: Always run `start-api.bat` to enable persistence.

## Migration from localStorage

If you already have scrapers in localStorage, they'll **automatically migrate**:

1. Start API server: `start-api.bat`
2. Open extension
3. Click any scraper to load it
4. Extension automatically saves to database
5. All future scrapers auto-saved

Or manually sync all:
```javascript
// In browser console
await syncScrapersToDatabase();
```

## Backup and Restore

### Export All Data
```javascript
// In extension
// Click "Export All Scrapers" button
// Downloads: scrapers-backup-2025-12-31.json
```

### Import Data
```javascript
// In extension
// Click "Import Scrapers" button
// Select: scrapers-backup-2025-12-31.json
```

### Database Backup
```bash
# PostgreSQL backup
pg_dump -U postgres civitracker > backup.sql

# Restore
psql -U postgres civitracker < backup.sql
```

## Monitoring

### Check What's Saved
```bash
# In psql
SELECT COUNT(*) FROM scraper_configs;
SELECT COUNT(*) FROM rag_episodes;
SELECT * FROM rag_domain_concepts;
```

### View in Extension
```javascript
// Browser console
const stats = await agent.enhancedMemory.getMemoryStats();
console.log(stats);
// Shows: totalEpisodes, withEmbeddings, concepts, successRate
```

## Performance

### Storage Size
- **localStorage**: ~450KB (200 episodes)
- **PostgreSQL**: ~50MB (unlimited episodes with compression)

### Sync Speed
- **Save scraper**: ~50ms
- **Save RAG episode**: ~100ms (with embedding)
- **Load all scrapers**: ~200ms

**Impact**: Negligible - you won't notice any slowdown.

## Summary

**Before:**
- âŒ Data lost on browser cache clear
- âŒ No cross-device sync
- âŒ Limited to browser storage (5-10MB)
- âŒ Manual export/import required

**After:**
- âœ… Permanent database storage
- âœ… Automatic syncing
- âœ… Cross-device sync capability
- âœ… Unlimited storage
- âœ… Backup and restore
- âœ… Works offline (falls back to localStorage)

**Just run `start-api.bat` and never worry about losing scrapers again!** ðŸŽ‰
