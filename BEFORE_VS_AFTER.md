# Before vs After: Architecture Comparison

## BEFORE (Single Backend - Development Only)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend  â”‚
â”‚  (React)   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â”‚ HTTP
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Netlify Dev       â”‚
â”‚   Port 8888         â”‚
â”‚                     â”‚
â”‚ Functions:          â”‚
â”‚ â€¢ state-events      â”‚â—€â”€â”€â”€ SCRAPES LIVE (slow!)
â”‚ â€¢ congress-meetings â”‚â—€â”€â”€â”€ SCRAPES LIVE (slow!)
â”‚ â€¢ local-meetings    â”‚â—€â”€â”€â”€ SCRAPES LIVE (slow!)
â”‚                     â”‚
â”‚ Optional:           â”‚
â”‚ PostgreSQL          â”‚
â”‚ (localhost)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âŒ Problems:
â€¢ Every API call triggers scraping (30+ seconds!)
â€¢ Rate limits hit frequently
â€¢ Expensive if deployed to cloud
â€¢ Single point of failure
```

## AFTER (Split Backend - Production Ready)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend  â”‚
â”‚  (React)   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â”‚ HTTP (fast!)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Backend       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   PostgreSQL     â”‚
â”‚   (Netlify)         â”‚  READ   â”‚   (Cloud)        â”‚
â”‚                     â”‚         â”‚                  â”‚
â”‚ Functions:          â”‚         â”‚  Events cached   â”‚
â”‚ â€¢ state-events      â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Updated daily   â”‚
â”‚ â€¢ congress-meetings â”‚         â”‚                  â”‚
â”‚ â€¢ local-meetings    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                     â”‚                  â”‚
â”‚ READ-ONLY           â”‚                  â”‚ WRITE
â”‚ (sub-100ms)         â”‚                  â”‚ (daily)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ Scraper Backend    â”‚
                              â”‚ (Your PC)          â”‚
                              â”‚                    â”‚
                              â”‚ Runs every 24h     â”‚
                              â”‚ Scrapes all states â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… Benefits:
â€¢ API responses < 100ms (just database reads)
â€¢ No rate limiting issues
â€¢ $0 hosting cost (scraping on your PC)
â€¢ Scalable to millions of users
â€¢ Reliable and maintainable
```

---

## Feature Comparison Table

| Feature | BEFORE (Dev Only) | AFTER (Production) |
|---------|-------------------|-------------------|
| **API Response Time** | 30+ seconds (scraping) | < 100ms (database) |
| **Cost (cloud hosting)** | $100-500/month | $0-20/month |
| **Scraping Frequency** | On every request | Every 24 hours |
| **Rate Limiting Issues** | Frequent | None |
| **Scalability** | Poor (scraping bottleneck) | Excellent (cached data) |
| **User Experience** | Slow, timeouts | Fast, reliable |
| **Data Freshness** | Real-time (but slow) | 24-hour cache |
| **Maintenance** | Complex (one codebase) | Simple (separated concerns) |
| **Development** | Easy (netlify dev) | Same! Still works locally |

---

## Code Location Changes

### BEFORE
```
netlify/functions/
â”œâ”€ state-events.ts          â—€â”€â”€ Scrapes AND serves
â”œâ”€ congress-meetings.ts     â—€â”€â”€ Scrapes AND serves
â”œâ”€ local-meetings.ts        â—€â”€â”€ Scrapes AND serves
â””â”€ scheduled-scraper.ts     â—€â”€â”€ Optional background job
```

### AFTER
```
netlify/functions/          â—€â”€â”€ API Backend (Cloud)
â”œâ”€ state-events.ts             âœ… Reads from DB only
â”œâ”€ congress-meetings.ts        âœ… Reads from DB only  
â”œâ”€ local-meetings.ts           âœ… Reads from DB only
â””â”€ [scheduled-scraper removed] âŒ Not needed in cloud

scraper-backend/            â—€â”€â”€ Scraper Backend (Your PC)
â”œâ”€ src/
â”‚  â”œâ”€ index.ts                 âœ… Cron scheduler
â”‚  â”œâ”€ scraper.ts               âœ… Runs all scrapers
â”‚  â””â”€ db/
â”‚     â”œâ”€ connection.ts         âœ… PostgreSQL pool
â”‚     â”œâ”€ events.ts             âœ… Insert events
â”‚     â””â”€ maintenance.ts        âœ… Cleanup old data
â””â”€ package.json
```

---

## Environment Variables

### BEFORE (Local Dev)
```bash
# .env (root directory)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=civitron
POSTGRES_USER=postgres
POSTGRES_PASSWORD=password
USE_POSTGRESQL=true
```

### AFTER (Production)

**Netlify (API Backend):**
```bash
# Netlify Dashboard â†’ Environment Variables
POSTGRES_HOST=db.supabase.co          # Cloud database
POSTGRES_PORT=5432
POSTGRES_DB=civitron
POSTGRES_USER=postgres
POSTGRES_PASSWORD=secure-cloud-pass
USE_POSTGRESQL=true
CONGRESS_API_KEY=xxx
OPENSTATES_API_KEY=xxx
```

**Scraper Backend (Your PC):**
```bash
# scraper-backend/.env
POSTGRES_HOST=db.supabase.co          # Same cloud database
POSTGRES_PORT=5432
POSTGRES_DB=civitron
POSTGRES_USER=postgres
POSTGRES_PASSWORD=secure-cloud-pass
CONGRESS_API_KEY=xxx
OPENSTATES_API_KEY=xxx
SCRAPE_INTERVAL_HOURS=24
```

---

## Daily Operation

### BEFORE
```
User visits site
    â†“
Clicks "New Hampshire"
    â†“
API function triggered
    â†“
Scrapes NH website (30+ seconds)
    â†“
User gets timeout or waits forever
    â†“
ğŸ˜ Bad experience
```

### AFTER
```
[3:00 AM - Automatic]
Scraper Backend wakes up
    â†“
Scrapes all 50 states (10 minutes)
    â†“
Writes to PostgreSQL
    â†“
Goes back to sleep

[Anytime - User request]
User visits site
    â†“
Clicks "New Hampshire"
    â†“
API reads from database (50ms)
    â†“
Returns cached events
    â†“
ğŸ˜Š Great experience!
```

---

## Migration Path

### Option 1: Keep Both (Recommended)

âœ… **Local Development**: Use existing setup
- `npm run netlify:dev`
- Scrapers run live (slow but accurate)
- Uses localhost PostgreSQL

âœ… **Production**: Use new split architecture
- Deploy API backend to Netlify
- Run scraper backend on your PC
- Uses cloud PostgreSQL

### Option 2: Production Only

If you don't need local development:
1. Deploy to production (cloud PostgreSQL + Netlify)
2. Run scraper backend on your PC
3. Use cloud URLs for testing

---

## What Stays the Same

âœ… All scraper code (reused from netlify/functions/utils/scrapers/)
âœ… Database schema (same tables, same structure)
âœ… Frontend code (no changes needed)
âœ… API endpoints (same URLs, same responses)
âœ… Local development workflow (netlify dev still works)

## What Changes

ğŸ”„ Production deployment (now two backends instead of one)
ğŸ”„ Scraping schedule (daily instead of on-demand)
ğŸ”„ Database location (cloud instead of localhost)
ğŸ”„ Cost structure (free instead of expensive)

---

## Key Files to Review

1. **`QUICK_START_PRODUCTION.md`** - Quick overview (read this first!)
2. **`PRODUCTION_DEPLOYMENT.md`** - Complete setup guide
3. **`scraper-backend/README.md`** - Scraper backend docs
4. **`ARCHITECTURE.txt`** - Visual architecture diagram
5. **`database/schema.sql`** - Database structure

---

## Summary

**The main idea:** Split heavy scraping work (runs on your PC daily) from lightweight API work (runs in cloud 24/7).

**Result:** Fast, reliable, free hosting! ğŸš€
