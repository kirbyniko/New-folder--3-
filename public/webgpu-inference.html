<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>WebGPU Inference Worker</title>
  <script type="module">
    // WebLLM - runs LLMs using WebGPU in browser
    import { CreateMLCEngine } from "https://esm.run/@mlc-ai/web-llm";

    let engine = null;
    let isInitialized = false;
    let isInitializing = false;

    // Initialize the engine with a small code-focused model
    async function initializeEngine() {
      if (isInitialized) {
        console.log('âœ… Engine already initialized, reusing existing instance');
        return;
      }
      if (isInitializing) {
        console.log('â³ Engine initialization already in progress, waiting...');
        // Wait for existing initialization
        while (isInitializing && !isInitialized) {
          await new Promise(resolve => setTimeout(resolve, 500));
        }
        return;
      }
      
      isInitializing = true;
      
      console.log('ðŸš€ Initializing WebGPU inference engine...');
      console.log('ðŸ“¦ Model will be cached in browser storage after first download');
      
      try {
        // Use Qwen2.5-Coder-7B - larger, more capable model for complex code (~4GB)
        // Model is cached in browser Cache Storage API - persists across sessions
        engine = await CreateMLCEngine("Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC", {
          initProgressCallback: (progress) => {
            const progressText = progress.text.includes('Fetching') 
              ? `${progress.text} (will be cached)`
              : progress.text;
            
            window.parent.postMessage({
              type: 'INIT_PROGRESS',
              progress: progress.progress,
              text: progressText
            }, '*');
          }
        });
        
        isInitialized = true;
        isInitializing = false;
        console.log('âœ… Engine initialized');
        
        window.parent.postMessage({
          type: 'INIT_COMPLETE',
          success: true
        }, '*');
        
      } catch (error) {
        console.error('âŒ Engine initialization failed:', error);
        isInitializing = false;
        window.parent.postMessage({
          type: 'INIT_COMPLETE',
          success: false,
          error: error.message
        }, '*');
      }
    }

    // Generate code based on prompt
    async function generateCode(prompt, options = {}) {
      if (!isInitialized) {
        await initializeEngine();
      }

      try {
        const messages = [
          {
            role: 'system',
            content: 'You are a code generation assistant. Output ONLY valid JavaScript code. No explanations, no markdown, no comments outside the code.'
          },
          {
            role: 'user',
            content: prompt
          }
        ];

        let fullResponse = '';
        let lastUpdateTime = Date.now();
        const startTime = Date.now();
        let stallCheckInterval;
        let completed = false;
        let tokenCount = 0;
        let firstTokenReceived = false;
        
        const completeGeneration = () => {
          if (completed) return;
          completed = true;
          
          if (stallCheckInterval) {
            clearInterval(stallCheckInterval);
          }
          
          window.parent.postMessage({
            type: 'GENERATION_COMPLETE',
            text: fullResponse,
            success: true
          }, '*');
        };
        
        const response = await engine.chat.completions.create({
          messages: messages,
          temperature: options.temperature || 0.1,
          max_tokens: options.max_tokens || 4000,
          stream: true,
        });

        // Check for stalls and timeout
        // Don't start checking for stalls until first token is received (model needs time to process prompt)
        stallCheckInterval = setInterval(() => {
          if (completed) {
            clearInterval(stallCheckInterval);
            return;
          }
          
          const timeSinceLastUpdate = Date.now() - lastUpdateTime;
          const totalTime = (Date.now() - startTime) / 1000;
          const charsPerSecond = tokenCount / totalTime;
          
          // If no tokens yet, allow up to 30 seconds for model to start
          if (!firstTokenReceived) {
            if (totalTime > 30) {
              console.warn('âš ï¸ Model failed to start generating (30s timeout)');
              completeGeneration();
            }
            return;
          }
          
          // Stall: no new tokens for 5 seconds (only after first token)
          if (timeSinceLastUpdate > 5000) {
            console.warn(`âš ï¸ Generation stalled (no tokens for 5s), completing with ${tokenCount} tokens`);
            completeGeneration();
          }
          // Timeout: taking too long overall (3 minutes for fixes)
          else if (totalTime > 180) {
            console.warn(`âš ï¸ Generation timeout (3 minutes), completing with ${tokenCount} tokens`);
            completeGeneration();
          }
          // Warn if slow but don't stop (let user know it's working)
          else if (totalTime > 20 && charsPerSecond < 2 && charsPerSecond > 0) {
            if (!this.slowWarningShown) {
              console.warn(`â±ï¸ Generation is slow (${charsPerSecond.toFixed(1)} chars/sec, ${tokenCount} tokens so far) but continuing...`);
              this.slowWarningShown = true;
            }
          }
        }, 1000);

        for await (const chunk of response) {
          if (completed) break;
          
          const delta = chunk.choices[0]?.delta?.content || '';
          if (delta) {
            if (!firstTokenReceived) {
              firstTokenReceived = true;
              console.log('ðŸŽ¯ First token received, generation active');
            }
            
            fullResponse += delta;
            tokenCount += delta.length;
            lastUpdateTime = Date.now();
            
            // Send progress updates
            window.parent.postMessage({
              type: 'GENERATION_PROGRESS',
              text: fullResponse
            }, '*');
          }
        }

        completeGeneration();

      } catch (error) {
        console.error('âŒ Generation failed:', error);
        window.parent.postMessage({
          type: 'GENERATION_COMPLETE',
          success: false,
          error: error.message
        }, '*');
      }
    }

    // Listen for messages from extension
    window.addEventListener('message', async (event) => {
      const { type, data } = event.data;

      switch (type) {
        case 'INIT_ENGINE':
          await initializeEngine();
          break;

        case 'GENERATE':
          await generateCode(data.prompt, data.options);
          break;

        case 'CHECK_STATUS':
          window.parent.postMessage({
            type: 'STATUS',
            isInitialized,
            hasWebGPU: !!navigator.gpu
          }, '*');
          break;
      }
    });

    // Check WebGPU availability on load
    if (!navigator.gpu) {
      window.parent.postMessage({
        type: 'ERROR',
        error: 'WebGPU not available in this browser. Please use Chrome/Edge 113+ with WebGPU enabled.'
      }, '*');
    } else {
      window.parent.postMessage({
        type: 'READY',
        hasWebGPU: true
      }, '*');
    }
  </script>
</head>
<body>
  <div style="font-family: monospace; padding: 20px;">
    <h3>WebGPU Inference Worker</h3>
    <p>Status: <span id="status">Waiting...</span></p>
    <p><small>This page uses WebGPU to run AI models locally in your browser.</small></p>
  </div>
</body>
</html>
